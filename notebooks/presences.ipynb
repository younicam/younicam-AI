{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following notebook perform a machine learning pipeline using TPOT on the presences.json file which list all the presences registered in the Unicam buildings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6322c893a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('younicam-AI').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - File preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data related to the registered presences which is composed of: \n",
    " - _id: the unique id given by MongoDB\n",
    " - aula: the room\n",
    " - polo: the building\n",
    " - sede: the city\n",
    " - posto: the seat\n",
    " - inDate: the datetime for the room access\n",
    " - outDate: the datetime for the room exit\n",
    " - date: the datetime for the last modification made on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>aula</th>\n",
       "      <th>date</th>\n",
       "      <th>inDate</th>\n",
       "      <th>outDate</th>\n",
       "      <th>polo</th>\n",
       "      <th>posto</th>\n",
       "      <th>sede</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5fa8ef7d1bd2a03f4641a15e</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-09T07:27:57.078Z</td>\n",
       "      <td>2020-11-09T07:27:57.078Z</td>\n",
       "      <td>2020-11-09T12:05:00.362Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fa8efa51bd2a03f4641a15f</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-09T07:28:37.074Z</td>\n",
       "      <td>2020-11-09T07:28:37.074Z</td>\n",
       "      <td>2020-11-09T12:05:00.363Z</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fa8f0751bd2a03f4641a160</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-09T07:32:05.879Z</td>\n",
       "      <td>2020-11-09T07:32:05.878Z</td>\n",
       "      <td>2020-11-09T12:05:00.364Z</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5fa8f0811bd2a03f4641a161</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-09T07:32:17.390Z</td>\n",
       "      <td>2020-11-09T07:32:17.390Z</td>\n",
       "      <td>2020-11-09T07:32:20.897Z</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5fa8f0891bd2a03f4641a162</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-09T07:32:25.980Z</td>\n",
       "      <td>2020-11-09T07:32:25.980Z</td>\n",
       "      <td>2020-11-09T07:32:36.245Z</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39359</th>\n",
       "      <td>5fd9dcb7ff3b76b96dd7987b</td>\n",
       "      <td>17</td>\n",
       "      <td>2020-12-16T10:08:55.168Z</td>\n",
       "      <td>2020-12-16T10:08:55.168Z</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39360</th>\n",
       "      <td>5fd9ddcfff3b76b96dd7988b</td>\n",
       "      <td>19</td>\n",
       "      <td>2020-12-16T10:13:35.299Z</td>\n",
       "      <td>2020-12-16T10:13:35.299Z</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39361</th>\n",
       "      <td>5fd9ddf1ff3b76b96dd7988e</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-16T10:14:09.471Z</td>\n",
       "      <td>2020-12-16T10:14:09.471Z</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39362</th>\n",
       "      <td>5fd9de6cff3b76b96dd79891</td>\n",
       "      <td>13</td>\n",
       "      <td>2020-12-16T10:16:12.267Z</td>\n",
       "      <td>2020-12-16T10:16:12.267Z</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39363</th>\n",
       "      <td>5fd9df86ff3b76b96dd79898</td>\n",
       "      <td>19</td>\n",
       "      <td>2020-12-16T10:20:54.612Z</td>\n",
       "      <td>2020-12-16T10:20:54.612Z</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39364 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _id aula                      date  \\\n",
       "0      5fa8ef7d1bd2a03f4641a15e    1  2020-11-09T07:27:57.078Z   \n",
       "1      5fa8efa51bd2a03f4641a15f    1  2020-11-09T07:28:37.074Z   \n",
       "2      5fa8f0751bd2a03f4641a160    1  2020-11-09T07:32:05.879Z   \n",
       "3      5fa8f0811bd2a03f4641a161    1  2020-11-09T07:32:17.390Z   \n",
       "4      5fa8f0891bd2a03f4641a162    1  2020-11-09T07:32:25.980Z   \n",
       "...                         ...  ...                       ...   \n",
       "39359  5fd9dcb7ff3b76b96dd7987b   17  2020-12-16T10:08:55.168Z   \n",
       "39360  5fd9ddcfff3b76b96dd7988b   19  2020-12-16T10:13:35.299Z   \n",
       "39361  5fd9ddf1ff3b76b96dd7988e    1  2020-12-16T10:14:09.471Z   \n",
       "39362  5fd9de6cff3b76b96dd79891   13  2020-12-16T10:16:12.267Z   \n",
       "39363  5fd9df86ff3b76b96dd79898   19  2020-12-16T10:20:54.612Z   \n",
       "\n",
       "                         inDate                   outDate polo posto sede  \n",
       "0      2020-11-09T07:27:57.078Z  2020-11-09T12:05:00.362Z    1     1    1  \n",
       "1      2020-11-09T07:28:37.074Z  2020-11-09T12:05:00.363Z    1     2    1  \n",
       "2      2020-11-09T07:32:05.878Z  2020-11-09T12:05:00.364Z    1     3    1  \n",
       "3      2020-11-09T07:32:17.390Z  2020-11-09T07:32:20.897Z    1     4    1  \n",
       "4      2020-11-09T07:32:25.980Z  2020-11-09T07:32:36.245Z    1     5    1  \n",
       "...                         ...                       ...  ...   ...  ...  \n",
       "39359  2020-12-16T10:08:55.168Z                      None   11    13    1  \n",
       "39360  2020-12-16T10:13:35.299Z                      None    5    12    1  \n",
       "39361  2020-12-16T10:14:09.471Z                      None    1    46    1  \n",
       "39362  2020-12-16T10:16:12.267Z                      None    9    15    4  \n",
       "39363  2020-12-16T10:20:54.612Z                      None    5    13    1  \n",
       "\n",
       "[39364 rows x 8 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = spark.read.json(\"../data/raw/presences.json\", multiLine=True)\n",
    "\n",
    "presencesDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform some operations to check the state of the data and change the names to improve readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('aula', 'string'),\n",
       " ('date', 'string'),\n",
       " ('inDate', 'string'),\n",
       " ('outDate', 'string'),\n",
       " ('polo', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('sede', 'string')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39364"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'date', 'inDate', 'outDate', 'building', 'seat', 'city']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.withColumnRenamed(\"aula\", \"room\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"polo\", \"building\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"sede\", \"city\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"posto\", \"seat\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look for null values inside each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>room</th>\n",
       "      <th>date</th>\n",
       "      <th>inDate</th>\n",
       "      <th>outDate</th>\n",
       "      <th>building</th>\n",
       "      <th>seat</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id  room  date  inDate  outDate  building  seat  city\n",
       "0    0     0     0       0      892         0     0     0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "presencesDF.select([count(when(isnull(c), c)).alias(c) for c in presencesDF.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see there are some null values inside the outDate column. This can happen because at the moment of data extraction there were some \"active\" presences that, of course cannot have the outDate. \n",
    "#### Those null values needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38472"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.replace('?', None).dropna(how='any')\n",
    "\n",
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the date column because it stores the date of the last modification made on the record and it is redundant since the last update made on the record is perfomed at the exit time that saved in the outDate field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'inDate', 'outDate', 'building', 'seat', 'city']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.drop(\"date\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast inDate and outDate into timestamp in order to extrapolate day, month, hour and minutes either for the entrance datetime and the exit datetime. \n",
    "#### Then the columns _id, posto, inDate and outDate_ are deleted since they are not needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', 'string'),\n",
       " ('building', 'string'),\n",
       " ('seat', 'string'),\n",
       " ('city', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"inDate\", presencesDF[\"inDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"outDate\", presencesDF[\"outDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"day\", dayofmonth(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"month\", month(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inHour\", hour(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inMinute\", minute(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outHour\", hour(presencesDF[\"outDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outMinute\", minute(presencesDF[\"outDate\"]))\n",
    "\n",
    "presencesDF = presencesDF.drop(\"_id\", \"posto\", \"inDate\", \"outDate\")\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast the column room, building and city to integer because the machine learning model works only with integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', 'int'),\n",
       " ('building', 'int'),\n",
       " ('seat', 'string'),\n",
       " ('city', 'int'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"room\", presencesDF[\"room\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"building\", presencesDF[\"building\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"city\", presencesDF[\"city\"].cast(IntegerType()))\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we need the number of people for a given room in a certain day and in a certain hour, we have to count the number of presences considering room, building, city, day, month, hour. \n",
    "#### In order to get the number of people present in a time interval, we can explode a sequence of hours (e.g. for a record with inHour: 8 and outHour 13, the sequence of hours will be: [8,9,10,11,12,13]), group by the hour (and other columns) and get the aggregate count for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room</th>\n",
       "      <th>building</th>\n",
       "      <th>city</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4563</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4567 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      room  building  city  day  month  hour  count\n",
       "0        5         3     1    9     11     8     16\n",
       "1       16         2     1   10     11    13      4\n",
       "2       11         6     1   10     11    15      4\n",
       "3       22         4     2   13     11    11     28\n",
       "4       26        14     2   13     11    11      4\n",
       "...    ...       ...   ...  ...    ...   ...    ...\n",
       "4562     1         1     1    1     12     9    168\n",
       "4563     9         4     2    2     12    17     64\n",
       "4564     1         1     1    3     12     8    188\n",
       "4565     7         7     1   14     12    17     36\n",
       "4566     1         1     1   15     12    14     88\n",
       "\n",
       "[4567 rows x 7 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "finalDF = presencesDF.withColumn(\n",
    "    'hour',\n",
    "    F.explode(F.sequence('inHour', 'outHour'))\n",
    ").groupBy(\n",
    "    'room', 'building', 'city', 'day', 'month', 'hour'\n",
    ").count()\n",
    "\n",
    "finalDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The generated column _count_ is our target features and all the others are the input values.\n",
    "#### The dataframe is ready for the ML model so we save it in a dedicated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.coalesce(1).write.format(\"json\").mode(\"overwrite\").save('../data/processed/presences.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room</th>\n",
       "      <th>building</th>\n",
       "      <th>city</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.323407</td>\n",
       "      <td>7.363039</td>\n",
       "      <td>1.423254</td>\n",
       "      <td>14.557697</td>\n",
       "      <td>11.400044</td>\n",
       "      <td>13.389096</td>\n",
       "      <td>35.751259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.275671</td>\n",
       "      <td>5.009367</td>\n",
       "      <td>0.785720</td>\n",
       "      <td>8.203988</td>\n",
       "      <td>0.489961</td>\n",
       "      <td>3.273675</td>\n",
       "      <td>41.163896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              room     building         city          day        month  \\\n",
       "count  4567.000000  4567.000000  4567.000000  4567.000000  4567.000000   \n",
       "mean     23.323407     7.363039     1.423254    14.557697    11.400044   \n",
       "std      17.275671     5.009367     0.785720     8.203988     0.489961   \n",
       "min       1.000000     1.000000     1.000000     1.000000    11.000000   \n",
       "25%       9.000000     3.000000     1.000000     9.000000    11.000000   \n",
       "50%      21.000000     7.000000     1.000000    14.000000    11.000000   \n",
       "75%      36.000000    12.000000     2.000000    20.000000    12.000000   \n",
       "max      68.000000    21.000000     4.000000    30.000000    12.000000   \n",
       "\n",
       "              hour        count  \n",
       "count  4567.000000  4567.000000  \n",
       "mean     13.389096    35.751259  \n",
       "std       3.273675    41.163896  \n",
       "min       0.000000     4.000000  \n",
       "25%      11.000000     8.000000  \n",
       "50%      13.000000    20.000000  \n",
       "75%      16.000000    48.000000  \n",
       "max      19.000000   256.000000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDF.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 1-D array containing the target values and the 2-D array with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = np.array(finalDF.select(\"count\").collect()).ravel()\n",
    "\n",
    "data = np.array(finalDF.select(\"room\", \"building\", \"city\", \"day\", \"month\", \"hour\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -50.68914859113579\n",
      "\n",
      "Generation 2 - Current best internal CV score: -50.68914859113579\n",
      "\n",
      "Generation 3 - Current best internal CV score: -50.42684286055529\n",
      "\n",
      "Generation 4 - Current best internal CV score: -50.42684286055529\n",
      "\n",
      "Generation 5 - Current best internal CV score: -50.15621012008095\n",
      "\n",
      "Generation 6 - Current best internal CV score: -50.15621012008095\n",
      "\n",
      "Generation 7 - Current best internal CV score: -49.87747294021666\n",
      "\n",
      "Generation 8 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 9 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 10 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 11 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 12 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 13 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 14 - Current best internal CV score: -49.31867454752522\n",
      "\n",
      "Generation 15 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 16 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 17 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 18 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 19 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 20 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 21 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 22 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 23 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 24 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 25 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 26 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 27 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 28 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 29 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 30 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 31 - Current best internal CV score: -48.57130501189921\n",
      "\n",
      "Generation 32 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 33 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 34 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 35 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 36 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 37 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 38 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 39 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 40 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 41 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 42 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 43 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 44 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 45 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 46 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 47 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 48 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 49 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 50 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 51 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 52 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 53 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 54 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 55 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 56 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 57 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 58 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 59 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 60 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 61 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 62 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 63 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 64 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 65 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 66 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 67 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 68 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 69 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 70 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 71 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 72 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 73 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 74 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 75 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 76 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 77 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 78 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 79 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 80 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 81 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 82 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 83 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 84 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 85 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 86 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 87 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 88 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 89 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 90 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 91 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 92 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 93 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 94 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 95 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 96 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 97 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 98 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 99 - Current best internal CV score: -47.52158656317905\n",
      "\n",
      "Generation 100 - Current best internal CV score: -47.52158656317905\n",
      "\n",
      "Best pipeline: RandomForestRegressor(SelectFromModel(SGDRegressor(RobustScaler(MinMaxScaler(input_matrix)), alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=invscaling, loss=squared_loss, penalty=elasticnet, power_t=0.1), max_features=0.9500000000000001, n_estimators=100, threshold=0.05), bootstrap=True, max_features=0.45, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
      "0.5314130926132525\n"
     ]
    }
   ],
   "source": [
    "/from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTRegressor(\n",
    "    verbosity=2,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "preds = tpot.predict(X_test)\n",
    "print(r2_score(y_test, preds))\n",
    "\n",
    "tpot.export('tpot_exported_pipeline.py')\n",
    "\n",
    "np.savetxt(\"../prediction/preds.csv\", preds, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution of TPOT outputs RandomForestRegressor as the best algorithm with also a python file to execute this algorithm. Below, the execution of RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5805560845350553"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(data, target, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: -47.52158656317905\n",
    "exported_pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    RobustScaler(),\n",
    "    StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=\"invscaling\", loss=\"squared_loss\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "    SelectFromModel(estimator=ExtraTreesRegressor(max_features=0.9500000000000001, n_estimators=100), threshold=0.05),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=0.45, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "exported_pipeline.score(testing_features, testing_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
