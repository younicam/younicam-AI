{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f02512fb2e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('younicam-AI').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data related to the registered presences which is composed of: \n",
    " - _id: the unique id given by MongoDB\n",
    " - aula: the room\n",
    " - polo: the building\n",
    " - sede: the city\n",
    " - inDate: the datetime for the room access\n",
    " - outDate: the datetime for the room exit\n",
    " - date: the datetime for the last modification made on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "|                 _id|aula|                date|              inDate|             outDate|polo|posto|sede|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "|5fa8ef7d1bd2a03f4...|   1|2020-11-09T07:27:...|2020-11-09T07:27:...|2020-11-09T12:05:...|   1|    1|   1|\n",
      "|5fa8efa51bd2a03f4...|   1|2020-11-09T07:28:...|2020-11-09T07:28:...|2020-11-09T12:05:...|   1|    2|   1|\n",
      "|5fa8f0751bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T12:05:...|   1|    3|   1|\n",
      "|5fa8f0811bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T07:32:...|   1|    4|   1|\n",
      "|5fa8f0891bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T07:32:...|   1|    5|   1|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presencesDF = spark.read.json(\"./data/presences.json\", multiLine=True)\n",
    "\n",
    "presencesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform some operations to check the state of the data and change the names to improve readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'date', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'date', 'inDate', 'outDate', 'building', 'posto', 'city']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.withColumnRenamed(\"aula\", \"room\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"polo\", \"building\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"sede\", \"city\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('room', 'string'),\n",
       " ('date', 'string'),\n",
       " ('inDate', 'string'),\n",
       " ('outDate', 'string'),\n",
       " ('building', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('city', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for null values inside each columns and, if present, delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+-------+--------+-----+----+\n",
      "|_id|room|date|inDate|outDate|building|posto|city|\n",
      "+---+----+----+------+-------+--------+-----+----+\n",
      "|  0|   0|   0|     0|    223|       0|    0|   0|\n",
      "+---+----+----+------+-------+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "presencesDF.select([count(when(isnull(c), c)).alias(c) for c in presencesDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9618"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.replace('?', None).dropna(how='any')\n",
    "\n",
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the column date because stores just the date of the last modification made on the record, so it is redundant since the last modification made on the record is the exit saved with outDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'inDate', 'outDate', 'building', 'posto', 'city']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.drop(\"date\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast inDate and outDate into timestamp in order to extrapolate day, month, hour and minutes. Then, delete not needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room',\n",
       " 'building',\n",
       " 'city',\n",
       " 'day',\n",
       " 'month',\n",
       " 'inHour',\n",
       " 'inMinute',\n",
       " 'outHour',\n",
       " 'outMinute']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"inDate\", presencesDF[\"inDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"outDate\", presencesDF[\"outDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"day\", dayofmonth(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"month\", month(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inHour\", hour(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inMinute\", minute(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outHour\", hour(presencesDF[\"outDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outMinute\", minute(presencesDF[\"outDate\"]))\n",
    "\n",
    "presencesDF = presencesDF.drop(\"_id\", \"posto\", \"inDate\", \"outDate\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast the column room, building and city into integer because the machine learning works only with integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', 'int'),\n",
       " ('building', 'int'),\n",
       " ('city', 'int'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"room\", presencesDF[\"room\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"building\", presencesDF[\"building\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"city\", presencesDF[\"city\"].cast(IntegerType()))\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to get the number of people present in a time interval, we can explode a sequence of hours (e.g. for a record with inHour: 8 and outHour 13, the sequence of hours will be: [8,9,10,11,12,13]), group by the hour (and other columns) and get the aggregate count for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+---+-----+----+-----+\n",
      "|room|building|city|day|month|hour|count|\n",
      "+----+--------+----+---+-----+----+-----+\n",
      "|   5|       3|   1|  9|   11|   8|    4|\n",
      "|  16|       2|   1| 10|   11|  13|    1|\n",
      "|  11|       6|   1| 10|   11|  15|    1|\n",
      "|  22|       4|   2| 13|   11|  11|    7|\n",
      "|  26|      14|   2| 13|   11|  11|    1|\n",
      "+----+--------+----+---+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "finalDF = presencesDF.withColumn(\n",
    "    'hour',\n",
    "    F.explode(F.sequence('inHour', 'outHour'))\n",
    ").groupBy(\n",
    "    'room', 'building', 'city', 'day', 'month', 'hour'\n",
    ").count()\n",
    "\n",
    "finalDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 1-D array containing the target values and the 2-D array with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = np.array(finalDF.select(\"count\").collect()).ravel()\n",
    "\n",
    "data = np.array(finalDF.select(\"room\", \"building\", \"city\", \"day\", \"month\", \"hour\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -50.68914859113579\n",
      "\n",
      "Generation 2 - Current best internal CV score: -50.68914859113579\n",
      "\n",
      "Generation 3 - Current best internal CV score: -50.42684286055529\n",
      "\n",
      "Generation 4 - Current best internal CV score: -50.42684286055529\n",
      "\n",
      "Generation 5 - Current best internal CV score: -50.15621012008095\n",
      "\n",
      "Generation 6 - Current best internal CV score: -50.15621012008095\n",
      "\n",
      "Generation 7 - Current best internal CV score: -49.87747294021666\n",
      "\n",
      "Generation 8 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 9 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 10 - Current best internal CV score: -49.68786907567703\n",
      "\n",
      "Generation 11 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 12 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 13 - Current best internal CV score: -49.552822076213126\n",
      "\n",
      "Generation 14 - Current best internal CV score: -49.31867454752522\n",
      "\n",
      "Generation 15 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 16 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 17 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 18 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 19 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 20 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 21 - Current best internal CV score: -49.311298586513225\n",
      "\n",
      "Generation 22 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 23 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 24 - Current best internal CV score: -49.24208559655734\n",
      "\n",
      "Generation 25 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 26 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 27 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 28 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 29 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 30 - Current best internal CV score: -49.028207947199846\n",
      "\n",
      "Generation 31 - Current best internal CV score: -48.57130501189921\n",
      "\n",
      "Generation 32 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 33 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 34 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 35 - Current best internal CV score: -48.44276023856237\n",
      "\n",
      "Generation 36 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 37 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 38 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 39 - Current best internal CV score: -48.227474045618195\n",
      "\n",
      "Generation 40 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 41 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 42 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 43 - Current best internal CV score: -48.00929695433176\n",
      "\n",
      "Generation 44 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 45 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 46 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 47 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 48 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 49 - Current best internal CV score: -47.99619915721834\n",
      "\n",
      "Generation 50 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 51 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 52 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 53 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 54 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 55 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 56 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 57 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 58 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 59 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 60 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 61 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 62 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 63 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 64 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 65 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 66 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 67 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 68 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 69 - Current best internal CV score: -47.87502758831945\n",
      "\n",
      "Generation 70 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 71 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 72 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 73 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 74 - Current best internal CV score: -47.757067169949906\n",
      "\n",
      "Generation 75 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 76 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 77 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 78 - Current best internal CV score: -47.71990797071406\n",
      "\n",
      "Generation 79 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 80 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 81 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 82 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 83 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 84 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 85 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 86 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 87 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 88 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 89 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 90 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 91 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 92 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 93 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 94 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 95 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 96 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 97 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 98 - Current best internal CV score: -47.662315539066526\n",
      "\n",
      "Generation 99 - Current best internal CV score: -47.52158656317905\n",
      "\n",
      "Generation 100 - Current best internal CV score: -47.52158656317905\n",
      "\n",
      "Best pipeline: RandomForestRegressor(SelectFromModel(SGDRegressor(RobustScaler(MinMaxScaler(input_matrix)), alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=invscaling, loss=squared_loss, penalty=elasticnet, power_t=0.1), max_features=0.9500000000000001, n_estimators=100, threshold=0.05), bootstrap=True, max_features=0.45, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
      "0.5314130926132525\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTRegressor(\n",
    "    verbosity=2,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "preds = tpot.predict(X_test)\n",
    "print(r2_score(y_test, preds))\n",
    "\n",
    "tpot.export('tpot_exported_pipeline.py')\n",
    "\n",
    "np.savetxt(\"prediction/preds.csv\", preds, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution of TPOT outputs RandomForestRegressor as the best algorithm with also a python file to execute this algorithm. Below, the execution of RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(data, target, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: -47.52158656317905\n",
    "exported_pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    RobustScaler(),\n",
    "    StackingEstimator(estimator=SGDRegressor(alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=\"invscaling\", loss=\"squared_loss\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "    SelectFromModel(estimator=ExtraTreesRegressor(max_features=0.9500000000000001, n_estimators=100), threshold=0.05),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=0.45, min_samples_leaf=1, min_samples_split=5, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "exported_pipeline.score(testing_features, testing_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
