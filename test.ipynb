{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fccf9d392e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('younicam-AI').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data related to the registered presences which is composed of: \n",
    " - _id: the unique id given by MongoDB\n",
    " - aula: the room\n",
    " - polo: the building\n",
    " - sede: the city\n",
    " - inDate: the datetime for the room access\n",
    " - outDate: the datetime for the room exit\n",
    " - date: the datetime for the last modification made on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "|                 _id|aula|                date|              inDate|             outDate|polo|posto|sede|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "|5fa8ef7d1bd2a03f4...|   1|2020-11-09T07:27:...|2020-11-09T07:27:...|2020-11-09T12:05:...|   1|    1|   1|\n",
      "|5fa8efa51bd2a03f4...|   1|2020-11-09T07:28:...|2020-11-09T07:28:...|2020-11-09T12:05:...|   1|    2|   1|\n",
      "|5fa8f0751bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T12:05:...|   1|    3|   1|\n",
      "|5fa8f0811bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T07:32:...|   1|    4|   1|\n",
      "|5fa8f0891bd2a03f4...|   1|2020-11-09T07:32:...|2020-11-09T07:32:...|2020-11-09T07:32:...|   1|    5|   1|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presencesDF = spark.read.json(\"./data/presences.json\", multiLine=True)\n",
    "\n",
    "presencesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform some operations to check the state of the data and change the names to improve readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'date', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'date', 'inDate', 'outDate', 'building', 'posto', 'city']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.withColumnRenamed(\"aula\", \"room\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"polo\", \"building\")\n",
    "presencesDF = presencesDF.withColumnRenamed(\"sede\", \"city\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('room', 'string'),\n",
       " ('date', 'string'),\n",
       " ('inDate', 'string'),\n",
       " ('outDate', 'string'),\n",
       " ('building', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('city', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for null values inside each columns and, if present, delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+-------+--------+-----+----+\n",
      "|_id|room|date|inDate|outDate|building|posto|city|\n",
      "+---+----+----+------+-------+--------+-----+----+\n",
      "|  0|   0|   0|     0|    223|       0|    0|   0|\n",
      "+---+----+----+------+-------+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "presencesDF.select([count(when(isnull(c), c)).alias(c) for c in presencesDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9618"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.replace('?', None).dropna(how='any')\n",
    "\n",
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the column date because stores just the date of the last modification made on the record, so it is redundant since the last modification made on the record is the exit saved with outDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'room', 'inDate', 'outDate', 'building', 'posto', 'city']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF = presencesDF.drop(\"date\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast inDate and outDate into timestamp in order to extrapolate day, month, hour and minutes. Then, delete not needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room',\n",
       " 'building',\n",
       " 'city',\n",
       " 'day',\n",
       " 'month',\n",
       " 'inHour',\n",
       " 'inMinute',\n",
       " 'outHour',\n",
       " 'outMinute']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"inDate\", presencesDF[\"inDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"outDate\", presencesDF[\"outDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"day\", dayofmonth(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"month\", month(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inHour\", hour(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inMinute\", minute(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outHour\", hour(presencesDF[\"outDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outMinute\", minute(presencesDF[\"outDate\"]))\n",
    "\n",
    "presencesDF = presencesDF.drop(\"_id\", \"posto\", \"inDate\", \"outDate\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast the column room, building and city into integer because the machine learning works only with integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('room', 'int'),\n",
       " ('building', 'int'),\n",
       " ('city', 'int'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"room\", presencesDF[\"room\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"building\", presencesDF[\"building\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"city\", presencesDF[\"city\"].cast(IntegerType()))\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to get the number of people present in a time interval, we can explode a sequence of hours (e.g. for a record with inHour: 8 and outHour 13, the sequence of hours will be: [8,9,10,11,12,13]), group by the hour (and other columns) and get the aggregate count for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+---+-----+----+-----+\n",
      "|room|building|city|day|month|hour|count|\n",
      "+----+--------+----+---+-----+----+-----+\n",
      "|   5|       3|   1|  9|   11|   8|    4|\n",
      "|  16|       2|   1| 10|   11|  13|    1|\n",
      "|  11|       6|   1| 10|   11|  15|    1|\n",
      "|  22|       4|   2| 13|   11|  11|    7|\n",
      "|  26|      14|   2| 13|   11|  11|    1|\n",
      "|   9|       4|   2| 16|   11|   9|   10|\n",
      "|   1|       1|   1| 19|   11|  11|   41|\n",
      "|  21|      16|   3| 24|   11|  18|    8|\n",
      "|  52|      10|   1| 24|   11|  19|    1|\n",
      "|   4|       4|   2| 24|   11|  15|   22|\n",
      "|  60|      11|   1| 25|   11|  19|   16|\n",
      "|  22|       4|   2| 25|   11|  17|    1|\n",
      "|  64|      12|   1| 30|   11|  12|   13|\n",
      "|   7|       7|   1| 30|   11|  16|   11|\n",
      "|  36|       1|   1|  1|   12|  13|    3|\n",
      "|   1|       5|   1|  1|   12|  19|   14|\n",
      "|   4|       4|   2|  1|   12|  18|   23|\n",
      "|  19|       5|   1|  2|   12|   9|   15|\n",
      "|  46|       1|   1|  2|   12|  15|    3|\n",
      "|  13|       9|   4|  2|   12|  19|   13|\n",
      "+----+--------+----+---+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "finalDF = presencesDF.withColumn(\n",
    "    'hour',\n",
    "    F.explode(F.sequence('inHour', 'outHour'))\n",
    ").groupBy(\n",
    "    'room', 'building', 'city', 'day', 'month', 'hour'\n",
    ").count()\n",
    "\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 1-D array containing the target values and the 2-D array with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = np.array(finalDF.select(\"count\").collect()).ravel()\n",
    "\n",
    "data = np.array(finalDF.select(\"room\", \"building\", \"city\", \"day\", \"month\", \"hour\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -105.42216235574115\n",
      "\n",
      "Generation 2 - Current best internal CV score: -104.62318388038477\n",
      "\n",
      "Generation 3 - Current best internal CV score: -104.16304407489217\n",
      "\n",
      "Generation 4 - Current best internal CV score: -102.43771964358295\n",
      "\n",
      "Generation 5 - Current best internal CV score: -96.32285518017609\n",
      "\n",
      "Best pipeline: DecisionTreeRegressor(AdaBoostRegressor(SelectFwe(input_matrix, alpha=0.02), learning_rate=0.1, loss=exponential, n_estimators=100), max_depth=10, min_samples_leaf=7, min_samples_split=18)\n",
      "0.48120333243312263\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,\n",
    "    population_size=100,\n",
    "    verbosity=2,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "preds = tpot.predict(X_test)\n",
    "print(r2_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
