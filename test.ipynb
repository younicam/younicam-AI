{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe117b22ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('younicam-AI').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "|_id                     |aula|date                    |inDate                  |outDate                 |polo|posto|sede|\n",
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "|5fa8ef7d1bd2a03f4641a15e|1   |2020-11-09T07:27:57.078Z|2020-11-09T07:27:57.078Z|2020-11-09T12:05:00.362Z|1   |1    |1   |\n",
      "|5fa8efa51bd2a03f4641a15f|1   |2020-11-09T07:28:37.074Z|2020-11-09T07:28:37.074Z|2020-11-09T12:05:00.363Z|1   |2    |1   |\n",
      "|5fa8f0751bd2a03f4641a160|1   |2020-11-09T07:32:05.879Z|2020-11-09T07:32:05.878Z|2020-11-09T12:05:00.364Z|1   |3    |1   |\n",
      "|5fa8f0811bd2a03f4641a161|1   |2020-11-09T07:32:17.390Z|2020-11-09T07:32:17.390Z|2020-11-09T07:32:20.897Z|1   |4    |1   |\n",
      "|5fa8f0891bd2a03f4641a162|1   |2020-11-09T07:32:25.980Z|2020-11-09T07:32:25.980Z|2020-11-09T07:32:36.245Z|1   |5    |1   |\n",
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presencesDF = spark.read.json(\"./data/presences.json\", multiLine=True)\n",
    "\n",
    "presencesDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'date', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('aula', 'string'),\n",
       " ('date', 'string'),\n",
       " ('inDate', 'string'),\n",
       " ('outDate', 'string'),\n",
       " ('polo', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('sede', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+-------+----+-----+----+\n",
      "|_id|aula|date|inDate|outDate|polo|posto|sede|\n",
      "+---+----+----+------+-------+----+-----+----+\n",
      "|  0|   0|   0|     0|    223|   0|    0|   0|\n",
      "+---+----+----+------+-------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking for null values\n",
    "\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "presencesDF.select([count(when(isnull(c), c)).alias(c) for c in presencesDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9618"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the null values founded\n",
    "\n",
    "presencesDF = presencesDF.replace('?', None).dropna(how='any')\n",
    "\n",
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary column\n",
    "\n",
    "presencesDF = presencesDF.drop(\"date\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('aula', 'string'),\n",
       " ('inDate', 'timestamp'),\n",
       " ('outDate', 'timestamp'),\n",
       " ('polo', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('sede', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast column inDate and outDate to timestamp\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"inDate\", presencesDF[\"inDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"outDate\", presencesDF[\"outDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aula', 'string'),\n",
       " ('polo', 'string'),\n",
       " ('sede', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('year', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the relevant content of inDate and outDate in different columns\n",
    "# drop the content of useless columns: _id, posto, inDate, outDate\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"day\", dayofmonth(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"month\", month(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"year\", year(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inHour\", hour(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inMinute\", minute(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outHour\", hour(presencesDF[\"outDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outMinute\", minute(presencesDF[\"outDate\"]))\n",
    "\n",
    "presencesDF = presencesDF.drop(\"_id\", \"posto\", \"inDate\", \"outDate\")\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|aula|polo|sede|day|month|year|inHour|inMinute|outHour|outMinute|\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|1   |1   |1   |9  |11   |2020|8     |27      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |28      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |8      |32       |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |8      |32       |\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cast the last string values to int and the _id to a progressive int id\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"aula\", presencesDF[\"aula\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"polo\", presencesDF[\"polo\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"sede\", presencesDF[\"sede\"].cast(IntegerType()))\n",
    "\n",
    "presencesDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+-----+\n",
      "|aula|polo|sede|day|month|year|count|\n",
      "+----+----+----+---+-----+----+-----+\n",
      "|  10|   5|   1| 25|   11|2020|    2|\n",
      "|  32|  13|   1| 26|   11|2020|    3|\n",
      "|  19|   5|   1| 26|   11|2020|   10|\n",
      "|   1|   1|   1| 27|   11|2020|   46|\n",
      "|  21|   1|   1|  3|   12|2020|    3|\n",
      "|  51|   7|   1| 19|   11|2020|    2|\n",
      "|   7|   7|   1| 23|   11|2020|   28|\n",
      "|  19|   5|   1| 25|   11|2020|   17|\n",
      "|  29|   7|   1| 26|   11|2020|    1|\n",
      "|   2|   2|   1| 27|   11|2020|    5|\n",
      "|  26|  14|   2| 11|   11|2020|    1|\n",
      "|   1|   5|   1|  9|   11|2020|   98|\n",
      "|  16|   2|   1| 24|   11|2020|    3|\n",
      "|  54|  12|   1| 30|   11|2020|    1|\n",
      "|  57|  12|   1| 20|   11|2020|    1|\n",
      "|  15|  10|   1| 24|   11|2020|    5|\n",
      "|  43|   7|   1|  1|   12|2020|   44|\n",
      "|   1|   1|   1|  3|   12|2020|   75|\n",
      "|   4|   4|   2| 14|   12|2020|   21|\n",
      "|  17|  11|   1|  9|   11|2020|   13|\n",
      "+----+----+----+---+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregation of data in order to create input columns (aula, polo, sede, day, month, year) and output column (count)\n",
    "\n",
    "aggregatedDF = presencesDF.groupBy(\"aula\", \"polo\", \"sede\", \"day\", \"month\", \"year\").count()\n",
    "\n",
    "aggregatedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+-----+--------------------+\n",
      "|aula|polo|sede|day|month|year|count|            features|\n",
      "+----+----+----+---+-----+----+-----+--------------------+\n",
      "|  10|   5|   1| 25|   11|2020|    2|[10.0,5.0,1.0,25....|\n",
      "|  32|  13|   1| 26|   11|2020|    3|[32.0,13.0,1.0,26...|\n",
      "|  19|   5|   1| 26|   11|2020|   10|[19.0,5.0,1.0,26....|\n",
      "|   1|   1|   1| 27|   11|2020|   46|[1.0,1.0,1.0,27.0...|\n",
      "|  21|   1|   1|  3|   12|2020|    3|[21.0,1.0,1.0,3.0...|\n",
      "|  51|   7|   1| 19|   11|2020|    2|[51.0,7.0,1.0,19....|\n",
      "|   7|   7|   1| 23|   11|2020|   28|[7.0,7.0,1.0,23.0...|\n",
      "|  19|   5|   1| 25|   11|2020|   17|[19.0,5.0,1.0,25....|\n",
      "|  29|   7|   1| 26|   11|2020|    1|[29.0,7.0,1.0,26....|\n",
      "|   2|   2|   1| 27|   11|2020|    5|[2.0,2.0,1.0,27.0...|\n",
      "|  26|  14|   2| 11|   11|2020|    1|[26.0,14.0,2.0,11...|\n",
      "|   1|   5|   1|  9|   11|2020|   98|[1.0,5.0,1.0,9.0,...|\n",
      "|  16|   2|   1| 24|   11|2020|    3|[16.0,2.0,1.0,24....|\n",
      "|  54|  12|   1| 30|   11|2020|    1|[54.0,12.0,1.0,30...|\n",
      "|  57|  12|   1| 20|   11|2020|    1|[57.0,12.0,1.0,20...|\n",
      "|  15|  10|   1| 24|   11|2020|    5|[15.0,10.0,1.0,24...|\n",
      "|  43|   7|   1|  1|   12|2020|   44|[43.0,7.0,1.0,1.0...|\n",
      "|   1|   1|   1|  3|   12|2020|   75|[1.0,1.0,1.0,3.0,...|\n",
      "|   4|   4|   2| 14|   12|2020|   21|[4.0,4.0,2.0,14.0...|\n",
      "|  17|  11|   1|  9|   11|2020|   13|[17.0,11.0,1.0,9....|\n",
      "+----+----+----+---+-----+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creation of features\n",
    "\n",
    "required_features = ['aula', 'polo', 'sede', 'day', 'month', 'year']\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = required_features,\n",
    "    outputCol = 'features')\n",
    "\n",
    "transformed_data = assembler.transform(aggregatedDF)\n",
    "\n",
    "transformed_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
