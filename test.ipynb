{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f61f404fac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('younicam-AI').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "|_id                     |aula|date                    |inDate                  |outDate                 |polo|posto|sede|\n",
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "|5fa8ef7d1bd2a03f4641a15e|1   |2020-11-09T07:27:57.078Z|2020-11-09T07:27:57.078Z|2020-11-09T12:05:00.362Z|1   |1    |1   |\n",
      "|5fa8efa51bd2a03f4641a15f|1   |2020-11-09T07:28:37.074Z|2020-11-09T07:28:37.074Z|2020-11-09T12:05:00.363Z|1   |2    |1   |\n",
      "|5fa8f0751bd2a03f4641a160|1   |2020-11-09T07:32:05.879Z|2020-11-09T07:32:05.878Z|2020-11-09T12:05:00.364Z|1   |3    |1   |\n",
      "|5fa8f0811bd2a03f4641a161|1   |2020-11-09T07:32:17.390Z|2020-11-09T07:32:17.390Z|2020-11-09T07:32:20.897Z|1   |4    |1   |\n",
      "|5fa8f0891bd2a03f4641a162|1   |2020-11-09T07:32:25.980Z|2020-11-09T07:32:25.980Z|2020-11-09T07:32:36.245Z|1   |5    |1   |\n",
      "+------------------------+----+------------------------+------------------------+------------------------+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presencesDF = spark.read.json(\"./data/presences.json\", multiLine=True)\n",
    "\n",
    "presencesDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'date', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('aula', 'string'),\n",
       " ('date', 'string'),\n",
       " ('inDate', 'string'),\n",
       " ('outDate', 'string'),\n",
       " ('polo', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('sede', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+-------+----+-----+----+\n",
      "|_id|aula|date|inDate|outDate|polo|posto|sede|\n",
      "+---+----+----+------+-------+----+-----+----+\n",
      "|  0|   0|   0|     0|    223|   0|    0|   0|\n",
      "+---+----+----+------+-------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking for null values\n",
    "\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "presencesDF.select([count(when(isnull(c), c)).alias(c) for c in presencesDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9618"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the null values founded\n",
    "\n",
    "presencesDF = presencesDF.replace('?', None).dropna(how='any')\n",
    "\n",
    "presencesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id', 'aula', 'inDate', 'outDate', 'polo', 'posto', 'sede']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary column\n",
    "\n",
    "presencesDF = presencesDF.drop(\"date\")\n",
    "\n",
    "presencesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('aula', 'string'),\n",
       " ('inDate', 'timestamp'),\n",
       " ('outDate', 'timestamp'),\n",
       " ('polo', 'string'),\n",
       " ('posto', 'string'),\n",
       " ('sede', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast column inDate and outDate to timestamp\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"inDate\", presencesDF[\"inDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"outDate\", presencesDF[\"outDate\"].cast(\"timestamp\"))\n",
    "\n",
    "presencesDF.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aula', 'string'),\n",
       " ('polo', 'string'),\n",
       " ('sede', 'string'),\n",
       " ('day', 'int'),\n",
       " ('month', 'int'),\n",
       " ('year', 'int'),\n",
       " ('inHour', 'int'),\n",
       " ('inMinute', 'int'),\n",
       " ('outHour', 'int'),\n",
       " ('outMinute', 'int')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the relevant content of inDate and outDate in different columns\n",
    "# drop the content of useless columns: _id, posto, inDate, outDate\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"day\", dayofmonth(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"month\", month(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"year\", year(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inHour\", hour(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"inMinute\", minute(presencesDF[\"inDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outHour\", hour(presencesDF[\"outDate\"]))\n",
    "presencesDF = presencesDF.withColumn(\"outMinute\", minute(presencesDF[\"outDate\"]))\n",
    "\n",
    "presencesDF = presencesDF.drop(\"_id\", \"posto\", \"inDate\", \"outDate\")\n",
    "\n",
    "presencesDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|aula|polo|sede|day|month|year|inHour|inMinute|outHour|outMinute|\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|1   |1   |1   |9  |11   |2020|8     |27      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |28      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |13     |5        |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |8      |32       |\n",
      "|1   |1   |1   |9  |11   |2020|8     |32      |8      |32       |\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cast the last string values to int and the _id to a progressive int id\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "presencesDF = presencesDF.withColumn(\"aula\", presencesDF[\"aula\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"polo\", presencesDF[\"polo\"].cast(IntegerType()))\n",
    "presencesDF = presencesDF.withColumn(\"sede\", presencesDF[\"sede\"].cast(IntegerType()))\n",
    "\n",
    "presencesDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+-----+\n",
      "|aula|polo|sede|day|month|year|count|\n",
      "+----+----+----+---+-----+----+-----+\n",
      "|  10|   5|   1| 25|   11|2020|    2|\n",
      "|  32|  13|   1| 26|   11|2020|    3|\n",
      "|  19|   5|   1| 26|   11|2020|   10|\n",
      "|   1|   1|   1| 27|   11|2020|   46|\n",
      "|  21|   1|   1|  3|   12|2020|    3|\n",
      "|  51|   7|   1| 19|   11|2020|    2|\n",
      "|   7|   7|   1| 23|   11|2020|   28|\n",
      "|  19|   5|   1| 25|   11|2020|   17|\n",
      "|  29|   7|   1| 26|   11|2020|    1|\n",
      "|   2|   2|   1| 27|   11|2020|    5|\n",
      "|  26|  14|   2| 11|   11|2020|    1|\n",
      "|   1|   5|   1|  9|   11|2020|   98|\n",
      "|  16|   2|   1| 24|   11|2020|    3|\n",
      "|  54|  12|   1| 30|   11|2020|    1|\n",
      "|  57|  12|   1| 20|   11|2020|    1|\n",
      "|  15|  10|   1| 24|   11|2020|    5|\n",
      "|  43|   7|   1|  1|   12|2020|   44|\n",
      "|   1|   1|   1|  3|   12|2020|   75|\n",
      "|   4|   4|   2| 14|   12|2020|   21|\n",
      "|  17|  11|   1|  9|   11|2020|   13|\n",
      "+----+----+----+---+-----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregated = presencesDF.groupBy(\"aula\", \"polo\", \"sede\", \"day\", \"month\", \"year\").count()\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|aula|polo|sede|day|month|year|inHour|inMinute|outHour|outMinute|\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "|   1|   1|   1|  9|   11|2020|     8|      27|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      28|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      32|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      32|      8|       32|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      32|      8|       32|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      36|      8|       53|\n",
      "|   2|   2|   1|  9|   11|2020|     8|      36|     11|        3|\n",
      "|   2|   2|   1|  9|   11|2020|     8|      36|     11|        3|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      45|      8|       52|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      45|      8|       52|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      45|     13|        5|\n",
      "|   3|   3|   1|  9|   11|2020|     8|      46|     13|        5|\n",
      "|   3|   3|   1|  9|   11|2020|     8|      46|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      47|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      47|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      47|     12|       44|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      49|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      49|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      50|     13|        5|\n",
      "|   1|   1|   1|  9|   11|2020|     8|      51|     13|        5|\n",
      "+----+----+----+---+-----+----+------+--------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "presencesDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
